{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.5007, 2.0173, 2.5367, 2.1667, 2.1191],\n",
       "         [2.0255, 2.4204, 2.9757, 2.7068, 2.9707]],\n",
       "\n",
       "        [[2.6206, 2.2528, 2.6710, 2.7278, 2.0203],\n",
       "         [2.6586, 2.4473, 2.6308, 2.7668, 2.8016]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.randn(2, 2, 5).uniform_(2,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1060,  1.0318,  2.0424],\n",
       "        [-1.2024,  1.1505, -0.6882]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3, 3)\n",
    "\n",
    "C = torch.matmul(A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5957, -0.0182, -0.0571],\n",
       "        [-0.0737,  0.2851,  1.1872]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dot product\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(2,3)\n",
    "C = A*B\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  6],\n",
      "        [ 6, 12]])\n",
      "tensor([[ 8, 13],\n",
      "        [11, 18]])\n"
     ]
    }
   ],
   "source": [
    "l = [[2,3],[3,4]]\n",
    "l1 = [[1,2],[2,3]]\n",
    "\n",
    "dot_l_l1 = torch.tensor(l)*torch.tensor(l1)\n",
    "print(dot_l_l1)\n",
    "\n",
    "mat_m_l_l1 = torch.matmul(torch.tensor(l),torch.tensor(l1))\n",
    "\n",
    "print(mat_m_l_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "l22 = list(mat_m_l_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 8, 13]), tensor([11, 18])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 13]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i have created my lists into tensor, multipleed them, now i want to convert them back into list\n",
    "\n",
    "l22 = l22[0].tolist()\n",
    "l22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(233)\n",
      "tensor(233)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(l22)\n",
    "b = torch.tensor(l22)\n",
    "\n",
    "print(a@b)\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7., 9.])\n"
     ]
    }
   ],
   "source": [
    "##gradient\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x**2 + 3*x\n",
    "z = y.sum()        # Scalar output required for backward\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/qv_lmdyx6j7b2zym7spyl3lm0000gn/T/ipykernel_88091/1324337507.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "##gradient\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x**2 + 3*x\n",
    "y.retain_grad()\n",
    "z = y.sum()        # Scalar output required for backward\n",
    "\n",
    "z.backward()\n",
    "print(y.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpput tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "output tensor([[-0.3232, -1.5775]], grad_fn=<AddmmBackward0>)\n",
      "output_shape torch.Size([1, 2])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## run this multiple time and you will see the magic of random initialization of weights and biases \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]], requires_grad=True)  # Input tensor with shape (1, 4)\n",
    "\n",
    "fc = nn.Linear(4, 2)  # Fully connected layer: input 4 → output 2\n",
    "#x = torch.randn(1, 4)  # Batch size 1, input size 4\n",
    "out = fc(x)\n",
    "print(\"inpput\", x)\n",
    "print(\"output\",out)\n",
    "print(\"output_shape\",out.shape)\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "## here the values are changing becuse of the random initialization of the weights and biases metrci\n",
    "# out = x*w = b\n",
    "# x - size - 1,4\n",
    "# w - size - 4,2\n",
    "# b - size - 1,2\n",
    "# out-size = 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpput tensor([[ 0.0924, -1.2100,  0.8146, -0.5334]])\n",
      "output_shape torch.Size([1, 2])\n",
      "output with relu tensor([[0.7556, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "output with sigmoid tensor([[0.6804, 0.4333]], grad_fn=<SigmoidBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# in the previous function we havn't applied activation\n",
    "# lets apply here\n",
    "#\n",
    "import torch.nn as nn\n",
    "\n",
    "fc = nn.Linear(4, 2)  # Fully connected layer: input 4 → output 2\n",
    "x = torch.randn(1, 4)  # Batch size 1, input size 4\n",
    "out = torch.relu(fc(x))  # Applying ReLU activation function\n",
    "out1 = torch.sigmoid(fc(x))  # Applying Sigmoid activation function\n",
    "print(\"inpput\", x)\n",
    "print(\"output_shape\",out.shape)\n",
    "print(\"output with relu\",out)\n",
    "print(\"output with sigmoid\",out1)\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "## here the values are changing becuse of the random initialization of the weights and biases metrci\n",
    "# out = x*w = b\n",
    "# x - size - 1,4\n",
    "# w - size - 4,2\n",
    "# b - size - 1,2\n",
    "# out-size = 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpput tensor([[1., 2., 3., 4.]], requires_grad=True)\n",
      "output_shape torch.Size([1, 2])\n",
      "output with relu tensor([[0.0174, 0.4451]], grad_fn=<ReluBackward0>)\n",
      "output with sigmoid tensor([[0.5044, 0.6095]], grad_fn=<SigmoidBackward0>)\n",
      "gradiend of x tensor([[ 0.2448,  0.1623,  0.1042, -0.0711]])\n"
     ]
    }
   ],
   "source": [
    "# in the previous function we havn't applied activation\n",
    "# lets apply here\n",
    "#\n",
    "import torch.nn as nn\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]], requires_grad=True)  # Input tensor with shape (1, 4)\n",
    "fc = nn.Linear(4, 2)  # Fully connected layer: input 4 → output 2\n",
    "# x = torch.randn(1, 4)  # Batch size 1, input size 4\n",
    "out = torch.relu(fc(x))  # Applying ReLU activation function\n",
    "out1 = torch.sigmoid(fc(x))  # Applying Sigmoid activation function\n",
    "print(\"inpput\", x)\n",
    "print(\"output_shape\",out.shape)\n",
    "print(\"output with relu\",out)\n",
    "print(\"output with sigmoid\",out1)\n",
    "\n",
    "out.sum().backward()\n",
    "print(\"gradiend of x\",x.grad)\n",
    "\n",
    "\n",
    "## here the values are changing becuse of the random initialization of the weights and biases metrci\n",
    "# out = x*w = b\n",
    "# x - size - 1,4\n",
    "# w - size - 4,2\n",
    "# b - size - 1,2\n",
    "# out-size = 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "inp = torch.randn(1, 2, 64, 64)\n",
    "out = conv(inp)\n",
    "print(out.shape)  # -> (1, 8, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Loss: 0.09280000627040863\n"
     ]
    }
   ],
   "source": [
    "## custom loss function\n",
    "\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def forward(self, pred, target, alpha):\n",
    "        return (1 - alpha) * torch.mean((pred - target)**2) + alpha * torch.mean(torch.abs(pred - target))\n",
    "\n",
    "\n",
    "pred = 0.2\n",
    "target = 0\n",
    "\n",
    "loss_fn = CustomMSELoss()\n",
    "loss = loss_fn(torch.tensor(pred), torch.tensor(target), 0.33)\n",
    "print(\"Custom Loss:\", loss.item())\n",
    "\n",
    "\n",
    "## why we are using only forward() inside even my custom loss\n",
    "## ans : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9914, -0.6948],\n",
       "         [-0.6798,  0.6187],\n",
       "         [ 0.1944, -1.3386]],\n",
       "\n",
       "        [[ 0.2082, -0.1028],\n",
       "         [ 1.5036,  0.9888],\n",
       "         [-0.8142, -1.7681]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(2, 3,2)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(8), tensor(13)]\n",
      "[tensor(11), tensor(18)]\n"
     ]
    }
   ],
   "source": [
    "for i in l22:\n",
    "    print(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9, 7, 1, 7, 8],\n",
       "         [1, 7, 2, 6, 3]],\n",
       "\n",
       "        [[1, 8, 7, 3, 1],\n",
       "         [4, 1, 0, 1, 8]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = torch.randint(0, 10, (2, 2, 5))\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
